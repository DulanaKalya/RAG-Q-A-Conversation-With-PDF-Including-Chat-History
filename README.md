# ğŸ“š RAG Q&A Conversation With PDF Uploads and Chat History

This project is an **end-to-end Conversational Retrieval-Augmented Generation (RAG)** system that allows users to upload **PDF documents** and ask questions based on the document content. The system remembers past interactions using **chat history**, improving multi-turn dialogue accuracy.

Built using **LangChain**, **Streamlit**, **Groq LLMs**, and **ChromaDB**, it showcases how to implement a document-aware, session-based chatbot using modern LLM infrastructure.

## ğŸ“ Project Structure

```
RAG-Q-A-Conversation.../           # âœ… Main RAG Q&A Project
â”œâ”€â”€ .env                           # HuggingFace token (environment variables)
â”œâ”€â”€ .gitignore                     # Git ignore file
â”œâ”€â”€ app.py                         # Main Streamlit app for RAG Q&A with chat history
â”œâ”€â”€ README.md                      # ğŸ“˜ Project documentation
â””â”€â”€ requirements.txt               # Required Python libraries
```

## ğŸš€ Features

* ğŸ—‚ï¸ Upload one or multiple PDF files
* ğŸ” Ask questions about the content in natural language
* ğŸ§  Uses Retrieval-Augmented Generation for accurate context-based answers
* ğŸ’¬ Maintains **session-based chat history** using `RunnableWithMessageHistory`
* ğŸ“š Reformulates questions based on chat history (history-aware retriever)
* ğŸ¤– Powered by **Groq LLM (Gemma-2-9b-IT)** and **MiniLM embeddings**
* ğŸŒ Simple and interactive UI built using **Streamlit**

## ğŸ› ï¸ Tech Stack & Tools

| Tool / Library | Purpose |
|---|---|
| `Streamlit` | Frontend UI |
| `LangChain` | RAG pipeline, prompts, history |
| `Groq` + `ChatGroq` | Fast LLM inference (Gemma 2 9B) |
| `HuggingFaceEmbeddings` | Embeddings via MiniLM |
| `Chroma` | In-memory vector database |
| `PyPDFLoader` | Load PDFs as LangChain docs |
| `RecursiveCharacterTextSplitter` | Chunking long documents |
| `dotenv` | API key management |

## ğŸ§ª How It Works

1. **User uploads PDF(s)**
2. Documents are loaded and split into chunks
3. Embeddings are created using `all-MiniLM-L6-v2`
4. Chunks are stored in a Chroma vector store
5. Questions are reformulated using chat history (`history-aware retriever`)
6. Final answers are generated using the RAG pipeline
7. Chat history is saved session-wise

## ğŸ–¼ï¸ Screenshots

*Add screenshots of the following:*

* ğŸ’¬ Asking a question
* ğŸ“œ Displaying answer and chat history
* ğŸ“Š Displaying langsmith results

You can add images like this:

```markdown
![Chat Interface](screenshots/1.png)
![Chat Interface](screenshots/3.png)
![Chat Interface](screenshots/2.png)


```

## ğŸ” Environment Setup

1. Clone the repo:

```bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Create a `.env` file with your Hugging Face token:

```env
HF_TOKEN=your_huggingface_token
```

4. Run the app:

```bash
streamlit run app.py
```

## ğŸ“Œ Requirements

* Python 3.8+
* Streamlit
* LangChain
* Groq API Key (you will input it in the app)
* HuggingFace Token for embeddings

## ğŸ“ˆ Example Use Cases

* Ask questions from academic research papers
* Query legal documents or contracts
* Summarize multi-page policy documents
* Interactive reading assistant for students

## ğŸ§  Future Improvements

* Add source document references with answers
* Allow multiple session management
* Integrate other LLM providers (OpenAI, Anthropic, etc.)
* UI enhancements for chat layout

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ using modern LLM and RAG technologies.